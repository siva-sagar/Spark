{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11ddf90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e0cffd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e35931f3",
   "metadata": {},
   "outputs": [],
   "source": [
    " emp_df = (spark\n",
    "          .read\n",
    "          .format(\"csv\")\n",
    "           .option(\"header\",\"true\")\n",
    "          .load(\"OneDrive\\Documents\\DATA\\employee.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c68d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+---------+-----------+\n",
      "| ID|   Name|   HomeTown|   Salary|JoiningDate|\n",
      "+---+-------+-----------+---------+-----------+\n",
      "|  1|  Arpit|  Burhanpur| 50000.00| 2018-11-14|\n",
      "|  2|   Benu|Bhubaneswar|100000.00| 2018-11-19|\n",
      "|  3|Dilsher|   Amritsar|100000.00| 2018-11-19|\n",
      "|  4|  Kiran|  Bengaluru| 50000.00| 2018-11-15|\n",
      "+---+-------+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6266770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_df = (spark\n",
    "          .read\n",
    "          .format(\"csv\")\n",
    "           .option(\"header\",\"true\")\n",
    "          .load(\"OneDrive\\Documents\\DATA\\department.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2ebaef3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------+-------------+\n",
      "|EmployeeID|  DepartmentName|        Client|OnboardedDate|\n",
      "+----------+----------------+--------------+-------------+\n",
      "|         1|Data Engineering|Funding Circle|   2019-01-15|\n",
      "|         2|Data Engineering|Funding Circle|   2018-11-19|\n",
      "|         3|  Data Analytics|Funding Circle|   2018-11-19|\n",
      "|         4|  Data Analytics|Funding Circle|   2018-12-16|\n",
      "+----------+----------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a9bc6",
   "metadata": {},
   "source": [
    "# Joining Data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1815c6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+---------+-----------+----------+----------------+--------------+-------------+\n",
      "| ID|   Name|   HomeTown|   Salary|JoiningDate|EmployeeID|  DepartmentName|        Client|OnboardedDate|\n",
      "+---+-------+-----------+---------+-----------+----------+----------------+--------------+-------------+\n",
      "|  1|  Arpit|  Burhanpur| 50000.00| 2018-11-14|         1|Data Engineering|Funding Circle|   2019-01-15|\n",
      "|  2|   Benu|Bhubaneswar|100000.00| 2018-11-19|         2|Data Engineering|Funding Circle|   2018-11-19|\n",
      "|  3|Dilsher|   Amritsar|100000.00| 2018-11-19|         3|  Data Analytics|Funding Circle|   2018-11-19|\n",
      "|  4|  Kiran|  Bengaluru| 50000.00| 2018-11-15|         4|  Data Analytics|Funding Circle|   2018-12-16|\n",
      "+---+-------+-----------+---------+-----------+----------+----------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = emp_df.join(dept_df,emp_df.ID == dept_df.EmployeeID)\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b2dda",
   "metadata": {},
   "source": [
    "#  Droping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d24ca1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+---------+-----------+----------------+--------------+-------------+\n",
      "| ID|   Name|   HomeTown|   Salary|JoiningDate|  DepartmentName|        Client|OnboardedDate|\n",
      "+---+-------+-----------+---------+-----------+----------------+--------------+-------------+\n",
      "|  1|  Arpit|  Burhanpur| 50000.00| 2018-11-14|Data Engineering|Funding Circle|   2019-01-15|\n",
      "|  2|   Benu|Bhubaneswar|100000.00| 2018-11-19|Data Engineering|Funding Circle|   2018-11-19|\n",
      "|  3|Dilsher|   Amritsar|100000.00| 2018-11-19|  Data Analytics|Funding Circle|   2018-11-19|\n",
      "|  4|  Kiran|  Bengaluru| 50000.00| 2018-11-15|  Data Analytics|Funding Circle|   2018-12-16|\n",
      "+---+-------+-----------+---------+-----------+----------------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = joined_df.drop(\"EmployeeID\")\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25860cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---------+-----------+----------------+-------------+\n",
      "| ID|   Name|   Salary|JoiningDate|  DepartmentName|OnboardedDate|\n",
      "+---+-------+---------+-----------+----------------+-------------+\n",
      "|  1|  Arpit| 50000.00| 2018-11-14|Data Engineering|   2019-01-15|\n",
      "|  2|   Benu|100000.00| 2018-11-19|Data Engineering|   2018-11-19|\n",
      "|  3|Dilsher|100000.00| 2018-11-19|  Data Analytics|   2018-11-19|\n",
      "|  4|  Kiran| 50000.00| 2018-11-15|  Data Analytics|   2018-12-16|\n",
      "+---+-------+---------+-----------+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = final_df.drop(\"HomeTown\", \"Client\")\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "75b8d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frames are immutable\n",
    "# we need to assign the changed to a new df they doesnot change inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08677d5",
   "metadata": {},
   "source": [
    "# User Defined Function\n",
    "\n",
    "### Does not use catalyst operator so not ideal, try using built in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "62258eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+---------+-----------+\n",
      "| ID|   Name|   HomeTown|   Salary|JoiningDate|\n",
      "+---+-------+-----------+---------+-----------+\n",
      "|  1|  Arpit|  Burhanpur| 50000.00| 2018-11-14|\n",
      "|  2|   Benu|Bhubaneswar|100000.00| 2018-11-19|\n",
      "|  3|Dilsher|   Amritsar|100000.00| 2018-11-19|\n",
      "|  4|  Kiran|  Bengaluru| 50000.00| 2018-11-15|\n",
      "+---+-------+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c5215a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf\n",
    "def fill_state(HomeTown):\n",
    "    if HomeTown == \"Bengaluru\":\n",
    "        return \"Karnataka\"\n",
    "    else:\n",
    "        return \"Out of State\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6fc1ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df = emp_df.withColumn(\"State\",fill_state(emp_df.HomeTown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "419380f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+---------+-----------+------------+\n",
      "| ID|   Name|   HomeTown|   Salary|JoiningDate|       State|\n",
      "+---+-------+-----------+---------+-----------+------------+\n",
      "|  1|  Arpit|  Burhanpur| 50000.00| 2018-11-14|Out of State|\n",
      "|  2|   Benu|Bhubaneswar|100000.00| 2018-11-19|Out of State|\n",
      "|  3|Dilsher|   Amritsar|100000.00| 2018-11-19|Out of State|\n",
      "|  4|  Kiran|  Bengaluru| 50000.00| 2018-11-15|   Karnataka|\n",
      "+---+-------+-----------+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0406bf81",
   "metadata": {},
   "source": [
    "# When and Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ded6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "emp_dep_final_df = emp_df.withColumn(\"State\", when(emp_df.HomeTown == \"Bengaluru\", \"Karnataka\").when(emp_df.HomeTown == \"Bhubaneswar\", \"Odisha\").otherwise(\"Out of state\"), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4916247a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----------+---------+-----------+------------+\n",
      "| ID|   Name|   HomeTown|   Salary|JoiningDate|       State|\n",
      "+---+-------+-----------+---------+-----------+------------+\n",
      "|  1|  Arpit|  Burhanpur| 50000.00| 2018-11-14|Out of state|\n",
      "|  2|   Benu|Bhubaneswar|100000.00| 2018-11-19|      Odisha|\n",
      "|  3|Dilsher|   Amritsar|100000.00| 2018-11-19|Out of state|\n",
      "|  4|  Kiran|  Bengaluru| 50000.00| 2018-11-15|   Karnataka|\n",
      "+---+-------+-----------+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dep_final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c625106a",
   "metadata": {},
   "source": [
    "# Writing the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "401e9e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriter at 0x211f319a160>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_dep_final_df.write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "75c2a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v emp_dep_final_df.write.parquet(\"C:\\data_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ddc8f06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp_dep_final_df.write.format(\"csv\").save(\"OneDrive\\Documents\\DATA\\emp_dep_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ebfe4e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emp_dep_final_df.write.format(\"csv\").option(\"header\",\"true\").save(\"OneDrive\\Documents\\DATA\\emp_dep_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "74ae0a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (emp_dep_final_df\n",
    "#  .write\n",
    "#  .format(\"csv\")\n",
    "#  .option(\"header\",\"true\")\n",
    "#  .mode(\"overwrite\")\n",
    "#  .option(\"compression\",\"snappy\")\n",
    "#  .save(\"OneDrive\\Documents\\DATA\\emp_dep_csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fe554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
